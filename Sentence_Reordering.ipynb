{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence Reconstruction","metadata":{"id":"ElNaMbLnRdHR"}},{"cell_type":"markdown","source":"The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n\nThe otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n\n\nCONSTRAINTS:\n* No pretrained model can be used.\n* The neural network models should have less the 20M parameters.\n* No postprocessing should be done (e.g. no beamsearch)\n* You cannot use additional training data.\n\n\nBONUS PARAMETERS:\n\nA bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters.","metadata":{"id":"oXr4iGUGRms8"}},{"cell_type":"markdown","source":"# Dataset\n\nThe dataset is composed by sentences taken from the generics_kb dataset of hugging face. We restricted the vocabolary to the 10K most frequent words, and only took sentences making use of this vocabulary.","metadata":{"id":"iQ8k-L-WUK7l"}},{"cell_type":"code","source":"!pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJ02vehGYySk","outputId":"a165d5cd-ec0d-4efd-9902-d9db99806756","execution":{"iopub.status.busy":"2024-06-12T07:17:08.254506Z","iopub.execute_input":"2024-06-12T07:17:08.254782Z","iopub.status.idle":"2024-06-12T07:17:21.752623Z","shell.execute_reply.started":"2024-06-12T07:17:08.254757Z","shell.execute_reply":"2024-06-12T07:17:21.751639Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade tensorflow\n!pip install --upgrade keras","metadata":{"id":"HKntF9du2oL7","execution":{"iopub.status.busy":"2024-06-12T07:17:21.754452Z","iopub.execute_input":"2024-06-12T07:17:21.754758Z","iopub.status.idle":"2024-06-12T07:18:41.084614Z","shell.execute_reply.started":"2024-06-12T07:17:21.754729Z","shell.execute_reply":"2024-06-12T07:18:41.083701Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nCollecting tensorflow\n  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nCollecting ml-dtypes~=0.3.1 (from tensorflow)\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.59.3)\nCollecting tensorboard<2.17,>=2.16 (from tensorflow)\n  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\nDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, tensorflow\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.1 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.16.1 which is incompatible.\ntf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ml-dtypes-0.3.2 tensorboard-2.16.2 tensorflow-2.16.1\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.3.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.10.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.11.0)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.3.2)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.9.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Download the dataset","metadata":{"id":"807Wk-ir_bDU"}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom keras.layers import TextVectorization\nimport tensorflow as tf\nimport numpy as np\nnp.random.seed(42)\nds = load_dataset('generics_kb',trust_remote_code=True)['train']","metadata":{"id":"_WjtqA8TrHcS","execution":{"iopub.status.busy":"2024-06-12T07:18:41.085983Z","iopub.execute_input":"2024-06-12T07:18:41.086307Z","iopub.status.idle":"2024-06-12T07:20:14.061187Z","shell.execute_reply.started":"2024-06-12T07:18:41.086277Z","shell.execute_reply":"2024-06-12T07:20:14.060450Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.64k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"721d3b467c89443c94c76b36d1f0221c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6674d75688eb43ea91cd7fec9196dc48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/27.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39ee9e0ace3489e8b9daab0526d0b7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1020868 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3576b8087a484f9394a5c464f676352e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Filter row with length greater than 8.\n","metadata":{"id":"lAVLfsdc_ej5"}},{"cell_type":"code","source":"ds = ds.filter(lambda row: len(row[\"generic_sentence\"].split(\" \")) > 8 )\ncorpus = [ '<start> ' + row['generic_sentence'].replace(\",\",\" <comma>\") + ' <end>' for row in ds ]\ncorpus = np.array(corpus)\n","metadata":{"id":"iznq8xGNt2Zr","execution":{"iopub.status.busy":"2024-06-12T07:20:14.063140Z","iopub.execute_input":"2024-06-12T07:20:14.063696Z","iopub.status.idle":"2024-06-12T07:21:26.645143Z","shell.execute_reply.started":"2024-06-12T07:20:14.063670Z","shell.execute_reply":"2024-06-12T07:21:26.644359Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1020868 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c83ddea5414498980dc776563b5480"}},"metadata":{}}]},{"cell_type":"markdown","source":"Create a tokenizer and Detokenizer","metadata":{"id":"FyYpXLCF_ldR"}},{"cell_type":"code","source":"tokenizer=TextVectorization( max_tokens=10000, standardize=\"lower_and_strip_punctuation\", encoding=\"utf-8\",) #con il max prende le piu frequenti. ordina i token del vocab dal piu frequente al meno frequente\ntokenizer.adapt(corpus)\n\nclass TextDetokenizer:\n    def __init__(self, vectorize_layer):\n        self.vectorize_layer = vectorize_layer\n        vocab = self.vectorize_layer.get_vocabulary()\n        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n\n    def __detokenize_tokens(self, tokens):\n        def check_token(t):\n          if t == 3:\n            s=\"<start>\"\n          elif t == 2:\n            s=\"<end>\"\n          elif t == 7:\n            s=\"<comma>\"\n          else:\n            s=self.index_to_word.get(t, '[UNK]')\n          return s\n\n        return ' '.join([ check_token(token) for token in tokens if token != 0])\n\n    def __call__(self, batch_tokens):\n       return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n\n\ndetokenizer = TextDetokenizer( tokenizer )\nsentences = tokenizer( corpus ).numpy()","metadata":{"id":"T-bE2JpVbU9E","execution":{"iopub.status.busy":"2024-06-12T07:21:26.646462Z","iopub.execute_input":"2024-06-12T07:21:26.646827Z","iopub.status.idle":"2024-06-12T07:21:34.225970Z","shell.execute_reply.started":"2024-06-12T07:21:26.646788Z","shell.execute_reply":"2024-06-12T07:21:34.225120Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Remove from corpus the sentences where any unknow word appears","metadata":{"id":"lZ64sns1_pSK"}},{"cell_type":"code","source":"mask = np.sum( (sentences==1), axis=1) >= 1\noriginal_data = np.delete( sentences, mask , axis=0)","metadata":{"id":"2LPQtryQz5wh","execution":{"iopub.status.busy":"2024-06-12T07:21:34.227380Z","iopub.execute_input":"2024-06-12T07:21:34.228192Z","iopub.status.idle":"2024-06-12T07:21:34.288110Z","shell.execute_reply.started":"2024-06-12T07:21:34.228150Z","shell.execute_reply":"2024-06-12T07:21:34.287284Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"original_data.shape","metadata":{"id":"qYfOscVk7U0r","execution":{"iopub.status.busy":"2024-06-12T07:21:34.289369Z","iopub.execute_input":"2024-06-12T07:21:34.289731Z","iopub.status.idle":"2024-06-12T07:21:34.296653Z","shell.execute_reply.started":"2024-06-12T07:21:34.289694Z","shell.execute_reply":"2024-06-12T07:21:34.295713Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(241236, 28)"},"metadata":{}}]},{"cell_type":"markdown","source":"Shuffle the sentences","metadata":{"id":"5puiiQ2D_uxa"}},{"cell_type":"code","source":"from tensorflow.keras.utils import PyDataset\n\nclass DataGenerator(PyDataset):\n    def __init__(self, data, batch_size=32, shuffle=True, seed=42):\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.seed = seed\n        self.on_epoch_end()\n\n\n    def __len__(self):\n        return int(np.floor(len(self.data) / self.batch_size))\n\n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        data_batch = np.array([self.data[k] for k in indexes])\n        #copy of ordered sequences\n        result = np.copy(data_batch)\n        #shuffle only the relevant positions for each batch\n        for i in range(data_batch.shape[0]):\n          np.random.shuffle(data_batch[i,1:data_batch[i].argmin() - 1])\n\n        return (data_batch , np.array([[result[i][j] for j in range(1,len(result[i]))] for i in range(len(result))] )), np.array([[result[i][j] for j in range(len(result[i])-1)] for i in range(len(result))])\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.data))\n        if self.shuffle:\n            if self.seed is not None:\n                np.random.seed(self.seed)\n            np.random.shuffle(self.indexes)","metadata":{"id":"1ZXLkWB6od0R","execution":{"iopub.status.busy":"2024-06-12T07:21:34.297675Z","iopub.execute_input":"2024-06-12T07:21:34.297947Z","iopub.status.idle":"2024-06-12T07:21:34.342049Z","shell.execute_reply.started":"2024-06-12T07:21:34.297923Z","shell.execute_reply":"2024-06-12T07:21:34.341395Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Make a random permutation of training and test set\nnp.random.seed(42)\n# Shuffle the all data\nshuffled_indices = np.random.permutation(len(original_data))\nshuffled_data = original_data[shuffled_indices]","metadata":{"id":"fNo_Jy3N8zHS","execution":{"iopub.status.busy":"2024-06-12T07:21:34.343006Z","iopub.execute_input":"2024-06-12T07:21:34.343284Z","iopub.status.idle":"2024-06-12T07:21:34.385103Z","shell.execute_reply.started":"2024-06-12T07:21:34.343255Z","shell.execute_reply":"2024-06-12T07:21:34.384053Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#split the dataset\ntrain_generator = DataGenerator(shuffled_data[:220000])\ntest_generator = DataGenerator(shuffled_data[220000:])","metadata":{"id":"uNlq1Khx1oH2","execution":{"iopub.status.busy":"2024-06-12T07:21:34.388491Z","iopub.execute_input":"2024-06-12T07:21:34.388793Z","iopub.status.idle":"2024-06-12T07:21:34.398692Z","shell.execute_reply.started":"2024-06-12T07:21:34.388768Z","shell.execute_reply":"2024-06-12T07:21:34.397813Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"(x, y), z = test_generator.__getitem__(1)\nx = detokenizer(x)\ny = detokenizer(y)\nz = detokenizer(z)\n\nfor i in range(7):\n  print(\"shuffled: \", x[i])\n  print(\"original shifted: \", y[i])\n  print(\"original: \", z[i])\n  print(\"\\n\")\n","metadata":{"id":"qR5xwMOn4E88","execution":{"iopub.status.busy":"2024-06-12T07:21:34.399832Z","iopub.execute_input":"2024-06-12T07:21:34.400088Z","iopub.status.idle":"2024-06-12T07:21:34.414897Z","shell.execute_reply.started":"2024-06-12T07:21:34.400065Z","shell.execute_reply":"2024-06-12T07:21:34.413930Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"shuffled:  <start> large their areas for cattle ranchers rainforest clear pastures become to of <end>\noriginal shifted:  ranchers clear large areas of rainforest to become pastures for their cattle <end>\noriginal:  <start> ranchers clear large areas of rainforest to become pastures for their cattle <end>\n\n\nshuffled:  <start> stripes thorax some and the earwigs on abdomen have <end>\noriginal shifted:  some earwigs have stripes on the thorax and abdomen <end>\noriginal:  <start> some earwigs have stripes on the thorax and abdomen <end>\n\n\nshuffled:  <start> into in magnetic such a liquid molecules can manipulation computing turn devices <end>\noriginal shifted:  magnetic manipulation can turn molecules in a liquid into computing such devices <end>\noriginal:  <start> magnetic manipulation can turn molecules in a liquid into computing such devices <end>\n\n\nshuffled:  <start> reduced wetlands and recreation for water places healthy cleaner flooding <comma> means more <end>\noriginal shifted:  healthy wetlands means cleaner water <comma> reduced flooding and more places for recreation <end>\noriginal:  <start> healthy wetlands means cleaner water <comma> reduced flooding and more places for recreation <end>\n\n\nshuffled:  <start> company percent share one controls a sales in market is share the particular in market <end>\noriginal shifted:  market share is the percent share in sales one company controls in a particular market <end>\noriginal:  <start> market share is the percent share in sales one company controls in a particular market <end>\n\n\nshuffled:  <start> of on animal only a the small flies time amount spend face <end>\noriginal shifted:  face flies spend only a small amount of time on the animal <end>\noriginal:  <start> face flies spend only a small amount of time on the animal <end>\n\n\nshuffled:  <start> extremely management in of foods are prevention and cancer important organic <end>\noriginal shifted:  organic foods are extremely important in prevention and management of cancer <end>\noriginal:  <start> organic foods are extremely important in prevention and management of cancer <end>\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"Fo8MazCGBTv3"}},{"cell_type":"markdown","source":"Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n\n1.  look for the longest substring w between s and p\n2.  compute |w|/max(|s|,|p|)\n\nIf the match is exact, the score is 1.\n\nWhen computing the score, you should NOT consider the start and end tokens.\n\n","metadata":{"id":"G0NOkuO0CfPo"}},{"cell_type":"markdown","source":"The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric.","metadata":{"id":"a-aUrdlXDdVf"}},{"cell_type":"code","source":"from difflib import SequenceMatcher\n\ndef score(s,p):\n  match = SequenceMatcher(None, s, p).find_longest_match()\n  #print(match.size)\n  return (match.size/max(len(p),len(s)))","metadata":{"id":"ulpTRdrF_huh","execution":{"iopub.status.busy":"2024-06-12T07:21:34.416240Z","iopub.execute_input":"2024-06-12T07:21:34.416608Z","iopub.status.idle":"2024-06-12T07:21:34.421667Z","shell.execute_reply.started":"2024-06-12T07:21:34.416583Z","shell.execute_reply":"2024-06-12T07:21:34.420760Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Let's do an example.","metadata":{"id":"RB2YfjXNExM-"}},{"cell_type":"code","source":"original = \"at first henry wanted to be friends with the king of france\"\ngenerated = \"henry wanted to be friends with king of france at the first\"\n\nprint(\"your score is \",score(original,generated))","metadata":{"id":"h17C8bVjEwur","execution":{"iopub.status.busy":"2024-06-12T07:21:34.422830Z","iopub.execute_input":"2024-06-12T07:21:34.423178Z","iopub.status.idle":"2024-06-12T07:21:34.435930Z","shell.execute_reply.started":"2024-06-12T07:21:34.423141Z","shell.execute_reply":"2024-06-12T07:21:34.434433Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"your score is  0.5423728813559322\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The score must be computed as an average of at least 3K random examples taken form the test set.","metadata":{"id":"BET8GqBvFugR"}},{"cell_type":"markdown","source":"# What to deliver","metadata":{"id":"4fwo7xj4GBW1"}},{"cell_type":"markdown","source":"You are supposed to deliver a single notebook, suitably commented.\nThe notebook should describe a single model, although you may briefly discuss additional attempts you did.\n\nThe notebook should contain a full trace of the training.\nWeights should be made available on request.\n\nYou must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n\n# Good work!","metadata":{"id":"i6uITuxOGHfJ"}},{"cell_type":"markdown","source":"# Solution","metadata":{}},{"cell_type":"markdown","source":"## Analysis of the benchmark value\n\nBefore we begin exposing the solution, we first have to set a benchmark value. We may consider as a baseline the results produced by a random shuffler: if the model is too close to a random solution we can't consider it successful enough.\nIn this portion of the code the entirety of the test set is processed by random shuffling the words of each sentence and computing the similarity score according to the metric provided.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy.random as nrd\nscores = []\nfor k in range(664):\n  (x, y), z = test_generator.__getitem__(k)\n  for e in range(len(x)):\n    nrd.shuffle(x[e])\n\n  for i in range(len(x)):\n    scores.append( score(detokenizer(y)[i], detokenizer(x)[i]))\n\nplt.hist(scores, bins=50)\nprint(f\"Standard Deviaiton is: {np.std(scores)}\")\nprint(f\"Average is: {np.average(scores)}\")\nprint(f\"Benchmark value is : {np.average(scores) + 3*np.std(scores)}\")","metadata":{"id":"IcKC9IATMv7w","execution":{"iopub.status.busy":"2024-06-12T07:21:34.437989Z","iopub.execute_input":"2024-06-12T07:21:34.439041Z","iopub.status.idle":"2024-06-12T07:22:07.358303Z","shell.execute_reply.started":"2024-06-12T07:21:34.438983Z","shell.execute_reply":"2024-06-12T07:22:07.357406Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Standard Deviaiton is: 0.04275765059502184\nAverage is: 0.15922296088781962\nBenchmark value is : 0.2874959126728851\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We'll consider the model far enough from the casual processing if its average scoring differs from the casual one by at least three times its standard deviation.\nAccording to this decision our benchmark value is: 0.287.","metadata":{}},{"cell_type":"markdown","source":"# Model Definition","metadata":{"id":"xTVgCp8976tK"}},{"cell_type":"markdown","source":"## The Architecture\n\nTo implement the solution the chosen architecture is the Transformer, one of the most famous models used for developing Sequence to Sequence operations. \nThe Transformer architecture is divided into three main components:\n* Encoder\n* Decoder\n* Feed-forward\n\nThe first two being the most important as they encapsulate entirely the mechanism of Attention, which is crucial to the effectiveness of the architecture. We will examine each layer and its internal structure in the following paragraphs.","metadata":{}},{"cell_type":"code","source":"LEN_VOC = 10000\nLEN_SENT = 28\nLEN_EMBED = 64\nLEN_TARGET_SENT= 27\nHEADS = 6\nLEN_FF = 256\nDROPOUT=0.01","metadata":{"id":"1ft2CGyr5SsH","execution":{"iopub.status.busy":"2024-06-12T07:22:07.359645Z","iopub.execute_input":"2024-06-12T07:22:07.360009Z","iopub.status.idle":"2024-06-12T07:22:07.365105Z","shell.execute_reply.started":"2024-06-12T07:22:07.359976Z","shell.execute_reply":"2024-06-12T07:22:07.364040Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"In the previous cell we find some structural constants which will be used during the development of the whole solution.\nWe have:\n* Length of the vocabulary\n* Length of the longest sentence we want to reorder\n* Length of the inner embedding inside the trasnformer\n* Length of the longest target sentence\n* Number of Heads in the Self Attention Layers\n* Size of the internal Feed-forward layers\n* Dropout rate","metadata":{}},{"cell_type":"markdown","source":"## Encoder","metadata":{}},{"cell_type":"markdown","source":"The encoder is the first main component of the Transformer Architecture, its role is the conversion of the tokenized input into an inner representation, to do so the inputs have to cross many inner layers. Inside the Encoder we find the following layers:\n* Token Embedding Layer, its task is to create an inner representation of the tokens inside the space with dimension LEN_EMBED\n* Positional Embedding Layer, its task is to provide the encoding with information about position by sampling function such as sine and cosine\n* 5 X Encoder Layer, enact the self attention mechanism inside the encoder, according to the most common architecture multiple of them were stacked to provide a better internal representation. Their structure will be analysed in the following cell.","metadata":{}},{"cell_type":"markdown","source":"### Encoder Layer","metadata":{}},{"cell_type":"markdown","source":"This is the main internal layer inside the Encoder Block and implments the self attention mechanism with multiple heads (6) in two separate layers.\nInside this complex layer we have different sublayers:\n* First Multi-head Attention layer, which applies the self attention mechanism to the inputs of the layer\n* First Normalization layer, which is introduced to contrast the phenomenon of exploding gradients using residual connections\n* Second Multi-head Attention layer, which applies the self attention mechanisms to the output of the normalization layer\n* Second Normalization layer, has the same function as the first one\n* Feed Forward Layer, it's a dense layer which helps in capturing eventual non linear relationships inside the model\n* Third Normalization Layer, has the same function as the other two\n* Dropout layer, introduced to prevent overfitting, it deactivates some units from previous layers to create different configurations each time.","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n     def __init__(self, num_heads, ff_size, embed_size):\n        super().__init__() \n        \n        #First self attention layer + normalization layer\n        self.multihead1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=LEN_EMBED)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        #Second self attention layer + normalization layer\n        self.multihead2= tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=LEN_EMBED)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        #Feed-forward dense layer + normalization layer\n        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_size, activation=\"leaky_relu\"), tf.keras.layers.Dense(LEN_EMBED),])\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        #Dropout layer\n        self.dropout = tf.keras.layers.Dropout(rate=DROPOUT)\n\n\n     def call(self, inputs):\n        \n\n        #Self Attention Section 1\n        attn_output_1 = self.multihead1(inputs, inputs)\n        out_1 = self.layernorm1(inputs + attn_output_1)\n        \n        \n        #Self Attention Section 2\n        attn_output_2 = self.multihead2(out_1, out_1)\n        out_2 = self.layernorm2(out_1 + attn_output_2)\n\n        #Feed Forward Section\n        ffn_output = self.ffn(out_2)\n        ffn_output = self.dropout(ffn_output)\n        out_3 = self.layernorm3(out_2 + ffn_output)\n\n        return out_3","metadata":{"id":"OxawHPkOkmEx","execution":{"iopub.status.busy":"2024-06-12T07:22:07.366139Z","iopub.execute_input":"2024-06-12T07:22:07.366471Z","iopub.status.idle":"2024-06-12T07:22:07.379084Z","shell.execute_reply.started":"2024-06-12T07:22:07.366447Z","shell.execute_reply":"2024-06-12T07:22:07.378276Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Complete Encoder","metadata":{}},{"cell_type":"markdown","source":"Here the complete encoder block is assembled, it extends the Layer class and reimplements two methods:\n* __init__ method to provide the new attributes (i.e. the sub-layers)\n* __call__ method which executes the internal steps of the encoder and successive processing of the inputs.","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_heads, ff_size, embed_size):\n    super().__init__()\n    \n    #Definition of the Token Embedding Layer\n    self.token_embedding = tf.keras.layers.Embedding(input_dim=LEN_VOC, output_dim=LEN_EMBED, mask_zero=True)\n    \n    #Definition of the Positional Embeding Layer\n    self.pos_embedding = tf.keras.layers.Embedding(input_dim=LEN_SENT, output_dim=LEN_EMBED)\n    \n    #Definition of the 5 internal EncoderLayer\n    self.encoder_1 = EncoderLayer(num_heads, ff_size, embed_size)\n    self.encoder_2 = EncoderLayer(num_heads, ff_size, embed_size)\n    self.encoder_3 = EncoderLayer(num_heads, ff_size, embed_size)\n    self.encoder_4 = EncoderLayer(num_heads, ff_size, embed_size)\n    self.encoder_5 = EncoderLayer(num_heads, ff_size, embed_size)\n    \n\n\n  def call(self, inputs):\n        \n        #Computing the token and positional embedding for the inputs\n        x=inputs\n        maxlen = tf.shape(x)[-1]\n        positions = tf.keras.ops.arange(start=0, stop=maxlen, step=1)\n        positions = self.pos_embedding(positions)\n        x = self.token_embedding(x)\n        out_1 = x + positions \n        \n        #Processing encoded inputs through the internal encoder layers\n        out_2 = self.encoder_1(out_1)\n        out_3 = self.encoder_2(out_2)\n        out_4 = self.encoder_3(out_3)\n        out_5 = self.encoder_4(out_4)\n        out_6 = self.encoder_5(out_5)\n        return out_6","metadata":{"id":"OxawHPkOkmEx","execution":{"iopub.status.busy":"2024-06-12T07:22:07.380473Z","iopub.execute_input":"2024-06-12T07:22:07.380855Z","iopub.status.idle":"2024-06-12T07:22:07.395904Z","shell.execute_reply.started":"2024-06-12T07:22:07.380825Z","shell.execute_reply":"2024-06-12T07:22:07.395016Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Decoder\n","metadata":{"id":"R3whFryPrBfP"}},{"cell_type":"markdown","source":"The decoder is the second component of the Transformer Architecture, its role is different during training and inference:\n* When training, the decoder receives as input the correct sequence with the objective of learning how to associate the inner representation produced by the encoder to the corect sequence.\n* During inference, the decoder will receive the starting token and iteratively produce the sequence of tokens that will constitute the complete, reordered sequence.\n\nThe internal structure of the decoder is very similar to the encoder with the introduction of one more layer inside the decoder layer:\n* Token Embedding Layer, its task is to create an inner representation of the input tokens inside the space with dimension LEN_EMBED\n* Positional Embedding Layer, its task is to provide the encoding with information about position by sampling function such as sine and cosine\n* 5 X Decoder Layer, enacts the self attention mechanism inside the encoder, according to the most common architecture multiple of them were used to provide a better internal representation. Their structure will be analysed in the following cell.","metadata":{}},{"cell_type":"markdown","source":"### Decoder Layer","metadata":{}},{"cell_type":"markdown","source":"This is the main internal layer inside the Decoder Block and implements the self attention mechanism with multiple heads (6) in three separate layers. Its structure is very reminiscent of the Encoder Layer, we have different sublayers:\n\n* First Multi-head Attention layer, it's the main difference from the encoder layer and uses a causal mask to prevent the model from looking at future tokens when predicting the next one, thus implementing the Causal Attention mechanism.\n* First Normalization layer, which is introduced to contrast the phenomenon of exploding gradients using residual connections\n* Second Multi-head Attention layer, which applies the self attention mechanism to the inputs of the layer\n* Second Normalization layer, serves the same function as the first\n* Third Multi-head Attention layer, which applies the self attention mechanisms to the output of the normalization layer\n* Third Normalization layer, has the same function as the first two\n* Feed Forward Layer, it's a dense layer which helps in capturing eventual non linear relationships inside the model\n* Third Normalization Layer, has the same function as the other three\n* Dropout layer, introduced to prevent overfitting, it deactivates some units from previous layers to create different configurations each time.","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, num_heads, ff_size, embed_size):\n        super().__init__() \n        \n        #Casual attention layer + normalization layer\n        self.multihead_mask = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=LEN_EMBED)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        #First self attention layer + normalization layer\n        self.multihead1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=LEN_EMBED)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        #Second self attention layer + normalization layer\n        self.multihead2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=LEN_EMBED)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        #Feed-forward dense layer + normalization layer\n        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_size, activation=\"leaky_relu\"), tf.keras.layers.Dense(LEN_EMBED),])\n        self.layernorm4 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n        #Dropout layer\n        self.dropout = tf.keras.layers.Dropout(rate=DROPOUT)\n\n     \n   \n    \n    \n    def call(self, encoder_out, decoder_inp_embed):\n        \n \n        causal_output = self.multihead_mask(decoder_inp_embed, decoder_inp_embed, use_causal_mask=True)\n        out_1 = self.layernorm1(decoder_inp_embed + causal_output)\n\n        #Self Attention Section 1\n        attn_output_1 = self.multihead1(out_1, encoder_out)\n        out_2= self.layernorm2(out_1 + attn_output_1)\n\n        #Self Attention Section 2\n        attn_output_2 = self.multihead2(out_2, encoder_out)\n        out_3 = self.layernorm3(out_2 + attn_output_2)\n\n        #Feed Forward Section\n        ffn_output = self.ffn(out_3)\n        ffn_output = self.dropout(ffn_output)\n        out_4 = self.layernorm4(out_3 + ffn_output)\n\n        return out_4","metadata":{"id":"5hvmCnfDoRFX","execution":{"iopub.status.busy":"2024-06-12T07:22:07.397457Z","iopub.execute_input":"2024-06-12T07:22:07.398021Z","iopub.status.idle":"2024-06-12T07:22:07.413400Z","shell.execute_reply.started":"2024-06-12T07:22:07.397985Z","shell.execute_reply":"2024-06-12T07:22:07.412397Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Complete Decoder","metadata":{}},{"cell_type":"markdown","source":"Here the complete decoder block is assembled, it extends the Layer class and reimplements two methods:\n* __init__ method to provide the new attributes (i.e. the sub-layers)\n* __call__ method which executes the internal steps of the decoder and successive processing of the inputs.\n\nInside the Decoder is also integrated the final Feed-Forward layer which will output the probabilites for each word predicted inside the dictionary.","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n  def __init__(self, num_heads, ff_size,embed_size):\n    super().__init__()\n    \n    #Definition of the Token Embedding Layer\n    self.token_embedding = tf.keras.layers.Embedding(input_dim=LEN_VOC, output_dim=LEN_EMBED, mask_zero=True)\n    \n    #Definition of the Positional Embeding Layer\n    self.pos_embedding = tf.keras.layers.Embedding(input_dim=LEN_TARGET_SENT, output_dim=LEN_EMBED)\n    \n    #Definition of the 5 internal EncoderLayer\n    self.decoder_1 = DecoderLayer(num_heads, ff_size, embed_size)\n    self.decoder_2 = DecoderLayer(num_heads, ff_size, embed_size)\n    self.decoder_3 = DecoderLayer(num_heads, ff_size, embed_size)\n    self.decoder_4 = DecoderLayer(num_heads, ff_size, embed_size)\n    self.decoder_5 = DecoderLayer(num_heads, ff_size, embed_size)\n    \n    #Definition of the output dense layer\n    self.outlayer = tf.keras.layers.Dense(LEN_VOC, activation='softmax')\n\n\n  def call(self, encoder_out, decoder_inp):\n            \n        #Computing the token and positional embedding for the inputs\n        x=decoder_inp\n        maxlen = tf.shape(x)[-1]\n        positions = tf.keras.ops.arange(start=0, stop=maxlen, step=1)\n        positions = self.pos_embedding(positions)\n        x = self.token_embedding(x)\n        out_1 = x + positions\n        \n        #Processing encoded inputs through the internal decoder layers\n        out_2 = self.decoder_1(encoder_out, out_1)\n        out_3 = self.decoder_2(encoder_out, out_2)\n        out_4 = self.decoder_3(encoder_out, out_3)\n        out_5 = self.decoder_4(encoder_out, out_4)\n        out_6 = self.decoder_5(encoder_out, out_5)\n        \n\n        #Returning the output through the dense layer\n        return self.outlayer(out_6)","metadata":{"id":"5hvmCnfDoRFX","execution":{"iopub.status.busy":"2024-06-12T07:22:07.414668Z","iopub.execute_input":"2024-06-12T07:22:07.414967Z","iopub.status.idle":"2024-06-12T07:22:07.429274Z","shell.execute_reply.started":"2024-06-12T07:22:07.414941Z","shell.execute_reply":"2024-06-12T07:22:07.428340Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Transformer","metadata":{"id":"OXDmjxuMuf1f"}},{"cell_type":"markdown","source":"The Trasformer class extends the Model class and joins Encoder and Decoder together, it redefines three methods:\n* __init__ method to set the Encoder and Decoder attributes\n* __call__ to apply the two inner complex layer to the inputs\n* __predict__ used during the testing phase to orchestrate the production of the output sequence token by token starting from the BOS token.","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, num_heads, ff_size, embed_size):\n            super().__init__()\n            self.encoder = Encoder(num_heads, ff_size, embed_size)\n    \n            self.decoder = Decoder(num_heads, ff_size, embed_size)\n\n            \n            \n\n    \n    def generate_initial_decoder_input(self, batch_size):\n        start_token = tf.constant([3], dtype=tf.int32)  # Assuming 3 is the start token\n        return tf.tile(tf.expand_dims(start_token, 0), [batch_size, 1])\n    \n    def call(self, encoder_inp, training):\n       \n        encoder_input, decoder_inp = encoder_inp\n        encoder_out = self.encoder(encoder_input)\n        \n        decoder_out = self.decoder(encoder_out, decoder_inp)\n        \n        return decoder_out\n    \n        \n    def predict(self, x, *args, **kwargs):\n        encoder_input, decoder_inputs = x\n\n        max_length = 28\n\n        batch_size = encoder_input.shape[0]\n        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n\n        start = np.array(tokenizer.word_index[''], ndmin=1)\n        output_array = output_array.write(0, tf.tile(start, [batch_size]))\n\n        for i in tf.range(max_length-1):\n            output = tf.transpose(output_array.stack())\n            predictions = self([encoder_input, output], training=False)\n\n            # Select the last token from the seq_len dimension.\n            predictions = predictions[:, -1:, :]  # Shape (batch_size, 1, vocab_size).\n\n            predicted_id = tf.argmax(predictions, axis=-1)\n\n            # Concatenate the predicted_id to the output which is given to the\n            # decoder as its input.\n            output_array = output_array.write(i+1, predicted_id[:, 0])\n        \n            end_mask = tf.reduce_any(tf.equal(predicted_id, tokenizer.word_index['']), axis=-1)\n            if tf.reduce_all(end_mask):\n                  break\n\n        output = tf.transpose(output_array.stack())\n        self([encoder_input, output[:,:-1]], training=False)\n      \n        return output      ","metadata":{"id":"fY-ILdAstWdU","execution":{"iopub.status.busy":"2024-06-12T07:23:02.655317Z","iopub.execute_input":"2024-06-12T07:23:02.655711Z","iopub.status.idle":"2024-06-12T07:23:02.669114Z","shell.execute_reply.started":"2024-06-12T07:23:02.655685Z","shell.execute_reply":"2024-06-12T07:23:02.668352Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Complete model","metadata":{}},{"cell_type":"markdown","source":"Here the entrire model is fully defined by providing the two input layers:\n* input which will feed directly into the encoder\n* target which will feed directly into the decoder during the training\n\nA summary of the model is produced to highlight the number of parameters before training.","metadata":{}},{"cell_type":"code","source":"training=False\ninputs = tf.keras.Input(shape=(LEN_SENT,))\ntarget = tf.keras.Input(shape=(LEN_TARGET_SENT,))\noutputs = Transformer(HEADS, LEN_FF, LEN_EMBED)(encoder_inp=[inputs, target], training=training)\nmodel = tf.keras.Model(inputs=[inputs, target], outputs=outputs)\nmodel.summary()","metadata":{"id":"pbOkPl86wEz_","execution":{"iopub.status.busy":"2024-06-12T07:23:04.992474Z","iopub.execute_input":"2024-06-12T07:23:04.992848Z","iopub.status.idle":"2024-06-12T07:23:13.060296Z","shell.execute_reply.started":"2024-06-12T07:23:04.992818Z","shell.execute_reply":"2024-06-12T07:23:13.059161Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_11\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m10000\u001b[0m) │  \u001b[38;5;34m4,756,880\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mTransformer\u001b[0m)       │                   │            │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,756,880</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)       │                   │            │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,756,880\u001b[0m (18.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,756,880</span> (18.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,756,880\u001b[0m (18.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,756,880</span> (18.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training the Model\n\nIn this part the model is trained using a split of the dataset of 220000 samples, 10000 of which were used for validation.\nThe choice for the optimizer is AdamW with a starting learning step of 0.00005, which is known as one of the best optimizer for transformer training.\n\nThe training goes on for a total of 10 epochs, the accuracy is evaluated throughout the whole process.\n\nAnother summary is printed to highlight the number of parameters after the training has concluded.","metadata":{}},{"cell_type":"code","source":"train_generator = DataGenerator(original_data[:210000], batch_size=32)\nvalidation_generator = DataGenerator(original_data[210000:220000], batch_size=32)\ntest_generator = DataGenerator(original_data[220000:], batch_size=32)\n\nopt = tf.keras.optimizers.AdamW(0.00005, gradient_accumulation_steps=4)\nmodel.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_generator, batch_size=32, epochs=10, validation_data=validation_generator)\n\nmodel.summary()","metadata":{"id":"-EUTl_-KiFNK","execution":{"iopub.status.busy":"2024-06-12T07:23:20.574364Z","iopub.execute_input":"2024-06-12T07:23:20.575077Z","iopub.status.idle":"2024-06-12T08:04:02.772854Z","shell.execute_reply.started":"2024-06-12T07:23:20.575043Z","shell.execute_reply":"2024-06-12T08:04:02.771900Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718177063.822682     127 service.cc:145] XLA service 0x7b3e4c0566b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1718177063.822730     127 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1718177063.822733     127 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nW0000 00:00:1718177066.056806     127 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718177091.305773     183 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_802', 108 bytes spill stores, 108 bytes spill loads\n\nI0000 00:00:1718177098.113585     183 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_804', 256 bytes spill stores, 256 bytes spill loads\n\nI0000 00:00:1718177128.070912     127 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m6561/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4710 - loss: 6.3223","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718177359.097242     127 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 36ms/step - accuracy: 0.4710 - loss: 6.3218 - val_accuracy: 0.6523 - val_loss: 2.8104\nEpoch 2/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 35ms/step - accuracy: 0.6784 - loss: 2.4616 - val_accuracy: 0.7535 - val_loss: 1.7738\nEpoch 3/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 36ms/step - accuracy: 0.7770 - loss: 1.5832 - val_accuracy: 0.8318 - val_loss: 1.2593\nEpoch 4/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 35ms/step - accuracy: 0.8595 - loss: 1.0920 - val_accuracy: 0.8908 - val_loss: 0.9022\nEpoch 5/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 35ms/step - accuracy: 0.9121 - loss: 0.7568 - val_accuracy: 0.9275 - val_loss: 0.6551\nEpoch 6/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 35ms/step - accuracy: 0.9444 - loss: 0.5291 - val_accuracy: 0.9525 - val_loss: 0.4774\nEpoch 7/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 35ms/step - accuracy: 0.9658 - loss: 0.3702 - val_accuracy: 0.9705 - val_loss: 0.3452\nEpoch 8/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 35ms/step - accuracy: 0.9809 - loss: 0.2546 - val_accuracy: 0.9806 - val_loss: 0.2526\nEpoch 9/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 36ms/step - accuracy: 0.9899 - loss: 0.1758 - val_accuracy: 0.9869 - val_loss: 0.1864\nEpoch 10/10\n\u001b[1m6562/6562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 35ms/step - accuracy: 0.9950 - loss: 0.1212 - val_accuracy: 0.9906 - val_loss: 0.1379\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_11\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m10000\u001b[0m) │  \u001b[38;5;34m4,756,880\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mTransformer\u001b[0m)       │                   │            │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,756,880</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)       │                   │            │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,027,522\u001b[0m (72.58 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,027,522</span> (72.58 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,756,880\u001b[0m (18.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,756,880</span> (18.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m14,270,642\u001b[0m (54.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,270,642</span> (54.44 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test the Model","metadata":{"id":"TizAn4lonaun"}},{"cell_type":"markdown","source":"In this last section the model is tested iterating over 21000 test samples, appending the results inside a list and then computing the average scoring. From what we can see, the average is way over the benchmark threshold, thus proving the success of the model.","metadata":{}},{"cell_type":"code","source":"#Testing the model\nscores_pred=[]\nEXAMPLE = 15\n#Cycling through all samples\nfor k in range(664):\n  x, y = test_generator.__getitem__(k)\n  predictions = model.predict(x, batch_size=32, verbose=False)\n  best = [[np.argmax(predictions[u][:][r]) for r in range(len(predictions[u]))] for u in range(len(predictions))]\n  for i in range(len(x)):\n    #Appending scores of the predictions\n    scores_pred.append(score(detokenizer(y)[i], detokenizer(best)[i]))\n\nprint(f\"Standard Deviaiton is: {np.std(scores_pred)}\")\nprint(f\"Average is: {np.average(scores_pred)}\")","metadata":{"id":"c7O2Vw9gv_YT","execution":{"iopub.status.busy":"2024-06-12T08:04:11.620814Z","iopub.execute_input":"2024-06-12T08:04:11.621746Z","iopub.status.idle":"2024-06-12T08:05:50.596138Z","shell.execute_reply.started":"2024-06-12T08:04:11.621694Z","shell.execute_reply":"2024-06-12T08:05:50.595158Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"I0000 00:00:1718179545.285928    5735 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_147', 108 bytes spill stores, 108 bytes spill loads\n\nI0000 00:00:1718179547.405883    5732 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_149', 108 bytes spill stores, 108 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"Standard Deviaiton is: 0.10241333716390603\nAverage is: 0.963303125945355\n","output_type":"stream"}]},{"cell_type":"code","source":"#Saving weights\nmodel.save_weights('Sentence_Reordering_weights.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:07:10.014286Z","iopub.execute_input":"2024-06-12T08:07:10.014657Z","iopub.status.idle":"2024-06-12T08:07:10.968839Z","shell.execute_reply.started":"2024-06-12T08:07:10.014628Z","shell.execute_reply":"2024-06-12T08:07:10.968051Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Model weights are saved.","metadata":{}}]}